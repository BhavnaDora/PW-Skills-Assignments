{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3757bbf-fee9-4e75-be13-97069e2fcda8",
   "metadata": {},
   "source": [
    "#### (1)-\n",
    "##### Regression is a type of Supervised Machine Learning algorithm. Simple Linear Regression is used when the dataset has 1 input feature and 1 output feature and there exists a linear relationship between the input and output feature. Example- Let's say our dataset has Weight and Height features where Weight is the only 1 input feature and Height is the output feature i.e. target variable. So, we use Simple Linear Regression to build and train model as well as predict the Height for new input data.\n",
    "##### Multiple Linear Regression is used when we have more than one input feature i.e multiple input features and 1 output feature and there exists a linear relationship between the input and output features. Example- Let's consider the Housing price dataset. Let it have features like Size of house, No. of rooms, Price. Size of house and No. of rooms are the 2 input features and Price is the output feature. So, we use Multiple Linear Regression to build and train model as well as predict the Price for new input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ffb33c-594d-4420-94cb-4ba3fcd84263",
   "metadata": {},
   "source": [
    "#### (2)-\n",
    "##### There are 5 assumptions of Linear Regression-\n",
    "1. Linear relationship between dependent and independent variable(s)- There should be a linear and additive relationship between dependent (output) variable and independent (input) variable(s). A linear relationship suggests that a change in Y due to one unit change in X1 is constant, regardless of the value of X1. An additive relationship suggests that the effect of X1 on Y is independent of other input variables.\n",
    "- How to check- The easiest way to detect if this assumption is met is to create a scatter plot of x vs. y. This allows you to visually see if there is a linear relationship between the two variables. If it looks like the points in the plot could fall along a straight line, then there exists some type of linear relationship between the two variables and this assumption is met.\n",
    "2. No correlation between the residual(error) terms- The residuals are assumed to be independent i.e no autocorrelation. The presence of correlation in error terms drastically reduces model’s accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. \n",
    "- How to check-  Look for Durbin – Watson (DW) statistic. It must lie between 0 and 4. If DW = 2, implies no autocorrelation, 0 < DW < 2 implies positive autocorrelation while 2 < DW < 4 indicates negative autocorrelation. Also, you can see residual vs time plot and look for the seasonal or correlated pattern in residual values.\n",
    "3. Independent variables should not be correlated- If the independent variables are correlated , the phenomena is called multicollinearity, it becomes difficult to find out which independent variable is actually contributing to predict the response variable.\n",
    "- How to check- we can use scatter plot to visualize correlation effect among independent variables. We can construct correlation table also to see if any correlation exists. Also, we can use VIF factor. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. \n",
    "4. Homoscedasticity- The error terms must have constant variance. Generally, non-constant variance, phenomena is called heteroscedasiticity arises in presence of outliers or extreme leverage values.\n",
    "- How to check- We can look at residual vs fitted values plot. If heteroskedasticity exists, the plot would exhibit a funnel shape pattern\n",
    "5. Normal distribution of error terms- We assume the residuals are normally distributed. \n",
    "- How to check- We can look at QQ plot. If the points on the plot roughly form a straight diagonal line, then the normality assumption is met. We can also perform statistical tests of normality such as Kolmogorov-Smirnov test, Shapiro-Wilk test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f19e59-a584-49e8-bccd-f8ecdad407a3",
   "metadata": {},
   "source": [
    "#### (3)-\n",
    "##### For a simple linear regression model, the best fit line is given by the equation- h_theta(x)= theta0 + theta1 * x where theta0 is the intercept, theta1 is the slope or coefficient.\n",
    "##### The intercept is the value on y axis where the best fit line passes through i.e. when x=0, the y value is the intercept. Slope is interpreted as the change of y for a one unit increase in x. \n",
    "##### Example- let's say in our dataset with experience and salary features where experience is the input feature and salary is the output feature, we get the equation of best fit line as- h_theta(x)= 20000 + 3000 * x \n",
    "- Intercept is 20000 which suggests that when experience is 0 , salary is 20000 i.e. the entry level salary in real world.\n",
    "- Slope is 3000 which is the estimated change in salary for one unit increase in experience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb54c50-3323-4909-9adf-92f1c805a105",
   "metadata": {},
   "source": [
    "#### (4)-\n",
    "##### Gradient Descent is an iterative optimization algorithm used to minimize the cost function of a machine learning model. The idea is to move in the direction of the steepest descent of the cost function to reach the global minimum or a local minimum. Here are the steps involved in the Gradient Descent algorithm:\n",
    "- First randomly initialize the parameters say theta0 and theta1 and compute the cost function.\n",
    "- At that point on the gradient descent curve, draw a tangent to compute the slope i.e. the gradient of cost function wrt the parameters. \n",
    "- Multiply this gradient of cost function with learning rate and subtract it from the parameters. Thus we updated the parameters and reduced cost function.\n",
    "- Repeat these steps until convergence i.e. till we reach the minimum cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5b1a70-4ec1-43aa-918e-dea2b8bb88d2",
   "metadata": {},
   "source": [
    "#### (5)-\n",
    "##### Multiple Linear Regression is one of the important regression algorithms which models the linear relationship between a single dependent continuous variable and more than one independent variable. Multiple Linear Regression is a generalization of Simple Linear Regression. The equation of the best fit plane is given as- \n",
    "- h_theta(x)= theta0 + theta1 * x1 + theta2 * x2 + .......... + thetaN * xN \n",
    "##### Simple Linear Regression is used for the dataset where there is 1 input feature and 1 output feature and there exists a linear relationship between the features. Whereas Multiple Linear Regression is used for datasets which have more than 1 input features i.e. multiple independent variables and 1 output feature. Here also linear relationship exists between the features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7654acd7-adc6-424c-b837-f1c914a952fd",
   "metadata": {},
   "source": [
    "#### (6)-\n",
    "##### In Multiple Linear Regression, multicollinearity occurs when two or more indpendent features have a high collinearity among themselves. When multicollinearity is present, the estimated regression coefficients may become large and unpredictable, leading to unreliable inferences about the effects of the predictor variables on the response variable. If the independent variables are correlated it becomes difficult to find out which independent variable is actually contributing to predict the response variable. Therefore, it is important to check for multicollinearity. Let's consider the equation- \n",
    "- Y = W0+ W1* X1+ W2* X2 \n",
    "- Coefficient W1 is the increase in Y for a unit increase in X1 while keeping X2 constant. But since X1 and X2 are highly correlated, changes in X1 would also cause changes in X2, and we would not be able to see their individual effect on Y.\n",
    "##### There are several ways to detect multicollinearity in a dataset, such as using the Variance Inflation Factor (VIF) or calculating the correlation matrix of the independent variables. VIF value <= 4 suggests no multicollinearity whereas a value of >= 10 implies serious multicollinearity. \n",
    "##### To address multicollinearity, techniques such as regularization or feature selection can be applied to select a subset of independent variables that are not highly correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb4e3b-b9eb-42e1-a404-036d46a06ad6",
   "metadata": {},
   "source": [
    "#### (7)-\n",
    "##### Polynomial Regression model is used for those datasets where the relationship between the input feature(s) and oitput feature is non linear. Using Polynomial Regression we obtain the best fit curve to fit the data. In polynomial regression, the relationship between the dependent variable and the independent variable is modeled as an nth-degree polynomial function. When the polynomial is of degree 1, it is a Simple Linear Regression model, when the polynomial is of degree 2, it is called a quadratic model; when the degree of a polynomial is 3, it is called a cubic model, and so on.\n",
    "- when the polynomial is of degree= n, the best fit curve is given by the equation- \n",
    "- h_theta(x)= theta0 + theta1 * x1^1 + theta2 * x1^2 + ...... + thetan * x1^n \n",
    "- the above equation is the general equation for Simple Polynomial Regression.\n",
    "##### Linear regression is used when we have linear relationship between input and output feature. Polynomial regression is used when we have a curvilinear or non linear relationship between input and output feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232e40aa-d439-4ae2-8166-bf68530bf4f5",
   "metadata": {},
   "source": [
    "#### (8)-\n",
    "##### Advantages of Polynomial Regression-\n",
    "- The polynomial regression is flexible enough to get fitted in a vast range of curvatures. \n",
    "- A broad range of functions can easily fit under it. \n",
    "- The polynomial regression offers the best approximation of the relationship between the dependent and independent variables. \n",
    "##### Disadvantages of Polynomial Regression-\n",
    "- The polynomial regression is very sensitive to the outliers. \n",
    "- The presence of one or more outliers in the data can affect the final results of the nonlinear analysis. \n",
    "- Very few model validation tools are available that help detect the outliers in nonlinear regression compared to the ones present for linear regression.\n",
    "- Overfitting and Underfitting situations might also arise.\n",
    "##### We build our Linear Regression model and realize that it performs abysmally. We examine the difference between the actual value and the best fit line we predicted, and it appears that the error is high and true value has a curve on the graph, but our line is nowhere near cutting the mean of the points. This is where polynomial regression comes into play; it predicts the best-fit line that matches the pattern of the data (curve). When the Linear Regression Model fails to capture the points in the data and the Linear Regression fails to adequately represent the optimum conclusion, Polynomial Regression is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f58be44-7ae6-469f-8c89-f3bb41dce05a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
