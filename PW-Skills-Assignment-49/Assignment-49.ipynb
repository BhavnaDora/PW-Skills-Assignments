{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb01c705-72bd-4d09-b25c-51e6f147b624",
   "metadata": {},
   "source": [
    "#### (1)-\n",
    "##### R-squared gives the degree of variability in the target variable that is explained by the model or the independent variables. \n",
    "- R-squared= 1- SSres/SStotal,    \n",
    "where SSres is the sum of squares of residual, SStotal is the sum of squares of total.\n",
    "- If we had a really low SSres value, it would mean that the regression line was very close to the actual points. This means the independent variables explain the majority of variation in the target variable. In such a case, we would have a really high R-squared value. \n",
    "- On the contrary, if we had a really high SSres value, it would mean that the regression line was far away from the actual points. Thus, independent variables fail to explain the majority of variation in the target variable. This would give us a really low R-squared value. \n",
    "##### So, this explains why the R-squared value gives us the variation in the target variable given by the variation in independent variables.\n",
    "##### R-squared value is basically a performance metric to know the accuracy of linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fbe31-9f1f-4e15-b662-06b1cdb17e83",
   "metadata": {},
   "source": [
    "#### (2)-\n",
    "##### The Adjusted R-squared takes into account the number of independent variables used for predicting the target variable. In doing so, we can determine whether adding new variables to the model actually increases the model fit.\n",
    "- Adjusted R-squared= 1- (1-R2)(n-1)/(n-p-1)       \n",
    "where R2 is R-squared value, n is the no. of data points in test set, p is the no. of independent features\n",
    "- If on adding a new independent variable we see a significant increase in R-squared value, then the Adjusted R-squared value will also increase indicating there is correlation between the new independent variable and output feature.\n",
    "- If R-squared does not increase significantly on the addition of a new independent variable, then the value of Adjusted R-squared will actually decrease indicating the new independent variable has little to no correlation with the output feature and is not important in predicting the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42e5d6d-d7f9-48f4-993a-3cbc42c7be77",
   "metadata": {},
   "source": [
    "#### (3)-\n",
    "##### The R-squared statistic isn’t perfect. In fact, it suffers from a major flaw. Its value never decreases no matter the number of variables we add to our regression model. That is, even if we are adding redundant variables to the data, the value of R-squared does not decrease. It either remains the same or increases with the addition of new independent variables. This clearly does not make sense because some of the independent variables might not be useful in determining the target variable. Adjusted R-squared deals with this issue. Adjusted R-squared is a better performance metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1414d-a162-45db-9c32-f5c77d10be0d",
   "metadata": {},
   "source": [
    "#### (4)- \n",
    "##### MSE is Mean Squared Error, MAE is Mean Absolute Error, RMSE is Root Mean Squared Error, all these are types of cost functions.\n",
    "- Mean Squared Error represents the average of the squared difference between the original and predicted values in the data set. It measures the variance of the residuals.   \n",
    "MSE= (Sum(y_actual- y_pred)^2)/n\n",
    "- The Mean absolute error represents the average of the absolute difference between the actual and predicted values in the dataset. It measures the average of the residuals in the dataset.            \n",
    "MAE= Sum(|y_actual- y_pred|)/n\n",
    "- Root Mean Squared Error is the square root of Mean Squared error. It measures the standard deviation of residuals.     \n",
    "RMSE= sqrt(MSE)\n",
    "##### The lower value of MAE, MSE, and RMSE implies lower error thus higher accuracy of a regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1df4c5-705e-4c79-9c86-4f83ca44d4af",
   "metadata": {},
   "source": [
    "#### (5)-\n",
    "##### Advantages and Disadvantages of MSE-\n",
    "1. Advantages-\n",
    "- It is differentiable\n",
    "- It has one global minima, i.e. its local minima and global minima are at the same point.\n",
    "2. Disadvantages-\n",
    "- MSE is not robust to outliers. Let's say our dataset has some outliers. As we find sum of squares of errors in MSE cost function, so in presence of outliers, the error will be high so to manage it best fit line gets shifted a lot.\n",
    "- It is not in the same unit as the output feature as we are squaring the errors.\n",
    "##### Advantages and Disadvantages of MAE-\n",
    "1. Advantages-\n",
    "- It is robust to outliers. If our dataset has some outliers, but as we only find the absolute value of error in MAE cost function, so our initial best fit line won't be shifted much as compared to MSE cost function.\n",
    "- It is in the same unit as the output feature.\n",
    "2. Disadvantages-\n",
    "- It is not differentiable at one point as it is a modulus function, so we take sub parts and compute gradient.\n",
    "- This results in slow convergence and complex optimization.\n",
    "##### Advantages and Disadvantages of RMSE-\n",
    "1. Advantages-\n",
    "- It is differentiable\n",
    "- It is in the same unit as the output feature\n",
    "2. Disadvantages-\n",
    "- It is not robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1becf8f-1f97-41d5-97b5-e737fd1467ec",
   "metadata": {},
   "source": [
    "#### (6)-\n",
    "##### LASSO stands for Least Absolute Shrinkage and Selection Operator. Lasso regularization is a type of regularization technique that works by penalizing the magnitude of coefficients of features and minimizing the error between predicted and actual observations. Lasso regression performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients to the cost function.\n",
    "- cost function= (sum(y_actual- y_pred)^2)/n  + lambda* (sum(|slope|)\n",
    "##### With increase in lambda (hyperparameter) the global minima shifts and the coefficient decreases. The key difference between ridge and lasso regression is that lasso regression tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. So, the coefficient that becomes 0 in lasso regression, that input feature can be dropped as it has little to 0 correlation with the output feature. So lasso regression also helps in feature selection.\n",
    "##### In cases where only a small number of predictor variables are significant, lasso regression tends to perform better because it’s able to shrink insignificant variables completely to zero and remove them from the model.\n",
    "##### However, when many predictor variables are significant in the model and their coefficients are roughly equal then ridge regression tends to perform better because it keeps all of the predictors in the model.\n",
    "##### To determine which model is better at making predictions, we typically perform k-fold cross-validation and choose whichever model produces the lowest test mean squared error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0708f3c2-6435-484e-b6d1-1d95c3b2b27c",
   "metadata": {},
   "source": [
    "#### (7)-\n",
    "##### If our best fit line passes through all the training data points then error or cost function is 0. When we apply this linear model to test data, it may perform bad and error will be high. This condition is called overfitting. It occurs when the model accuracy wrt train dataset is high but low wrt test dataset. To overcome this we use regularization techniques like Ridge regression and Lasso regression. We add penalty term to the cost function so that the error will not be 0 and it will avoid overfitting.\n",
    "##### Ridge regression uses L2 regularization to shrink the coefficients towards zero and prevent overfitting, resulting in a more robust and accurate model. The amount of regularization is controlled by the tuning parameter alpha, which balances the tradeoff between bias and variance.\n",
    "##### Let's consider an example of fitting a linear regression model to predict housing prices. We have a dataset of 1000 houses with 10 predictor variables such as size, number of rooms, and location, and the target variable is the sale price of the house. We split the dataset into a training set of 700 houses and a test set of 300 houses. We fit a regularized linear regression model using Ridge regularization with an alpha parameter of 0.1. This means that the penalty term added to the cost function is proportional to the square of the coefficients of the predictor variables, with a weight of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e861940-f980-4ec4-ba70-180baa89e283",
   "metadata": {},
   "source": [
    "#### (8)-\n",
    "##### Limitations of Ridge regression-\n",
    "- It includes all the predictors in the final model.\n",
    "- It is not capable of performing feature selection.\n",
    "- It trades variance for bias.\n",
    "- Ridge regression decreases the complexity of a model but does not reduce the number of variables since it never leads to a coefficient being zero rather only minimizes it. Hence, this model is not good for feature reduction.\n",
    "##### Limitations of Lasso regression-\n",
    "- If there are two or more highly collinear variables then Lasso regression select one of them randomly which is not good for the interpretation of data.\n",
    "- The coefficients that are produced by a Lasso model are biased. The L1 penalty that is added to the model artificially shrinks the coefficients closer to zero, or in some cases, all the way to zero. That means that the coefficients from a Lasso model do not represent the true magnitude of the relationship between the features and and the outcome, but rather a shrunken version of that magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc991f2-51f7-4430-a564-e4d36f5483d4",
   "metadata": {},
   "source": [
    "#### (9)-\n",
    "##### The model A has RMSE=10 and model B has MAE=8, based on the magnitude of these errors we should choose model B as the better performer because lower the error higher is the model's performance or accuracy. Also MAE is robust to outliers. Some disadvantages of MAE are-\n",
    "- It is not differentiable at one point as it is a modulus function, so we take sub parts and compute gradient.\n",
    "- This results in slow convergence and complex optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be3fb2e-d679-4e99-b6c3-6802af46bcf9",
   "metadata": {},
   "source": [
    "#### (10)-\n",
    "##### When we want to keep all the input or independent features of our dataset then we should choose model A which uses Ridge regression with a regularization parameter of 0.1. Ridge regression helps in shrinking the coefficient to reduce the error or cost function and also avoids overfitting. But in Ridge regression increasing the hyper parameter lambda will no doubt decrease the coefficient but it won't make it 0 .\n",
    "##### When we want to select those input features that have high correlation with the output feature and contribute more in predicting the output feature then we should choose model B which uses Lasso regression with a regularization parameter of 0.5. Lasso regression also helps in shrinking the coefficients to reduce the error. In lasso regression on increasing the hyper parameter lambda the coefficients decrease and become 0 too, so we can drop those features. Thus it helps in feature selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34e5aad-e330-4387-b4eb-11fce9a73330",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
