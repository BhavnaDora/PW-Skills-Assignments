{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955e5bba-f2c5-4a08-a518-cc7cc5a8db53",
   "metadata": {},
   "source": [
    "#### (1)-\n",
    "##### Overfitting is a situation when your model is too complex for your data. Overfitting is when the accuracy of model when tested on training dataset is high but the accuarcy of the model when tested on test dataset is low. So, it is characterized by low bias and high variance.\n",
    "##### Underfitting is a situation when your model is too simple for your data. Underfitting is when the training accuracy and testing accuracy is low. So, it is characterized by high bias and high variance.\n",
    "##### Consequences-\n",
    "##### Overfitting is when the model fits too closely to the training data, it is then unable to generalize well to new data and fails to perform accurately. \n",
    "##### Underfitting is when the model is uanble to capture the relationship between the input and output variables accurately, generating a high error rate on both the training set and unseen data, thus it cannot establish the dominant trend within the data, resulting in training errors and poor performance of the model. It is uanble to generalize well to new data and fails to perform accurately.\n",
    "##### Avoiding overfitting by- \n",
    "##### Early stopping\n",
    "##### Train with more data\n",
    "##### Data augmentation\n",
    "##### Feature selection\n",
    "##### Regularization\n",
    "\n",
    "##### Avoiding underfitting by- \n",
    "##### Feature selection\n",
    "##### Increase duration of training\n",
    "##### Decrease regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8d8fb6-c2a8-44c1-bd45-59301ecea8ae",
   "metadata": {},
   "source": [
    "#### (2)-\n",
    "##### We can reduce overfitting by these methods-\n",
    "##### 1- Stopping the training period early- We have to pause the training early before the model starts to learn the noise.\n",
    "##### 2- Training with more data- Expanding the training set with clean, relevant data can increase the accuracy. \n",
    "##### 3- Data augmentation- While it is better to inject clean, relevant data into your training data, sometimes noisy data is added to make a model more stable. However, this method should be done sparingly.\n",
    "##### 4- Feature selection- Feature selection is the process of identifying the most important features within the training data and then eliminating the irrelevant or redundant ones. This will reduce the complexity of model by simplifying it and thus mitigate overfitting.\n",
    "##### 5- Regularization- If we don’t know which features to remove from our model, regularization methods can be particularly helpful. Regularization applies a “penalty” to the input parameters with the larger coefficients, which subsequently limits the amount of variance in the model. While there are a number of regularization methods, such as L1 regularization, Lasso regularization, and dropout, they all seek to identify and reduce the noise within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10767052-ea63-48ec-8bac-8757bb3a37b3",
   "metadata": {},
   "source": [
    "#### (3)-\n",
    "##### Underfitting occurs when our model is too simple for the data. Our model is unable to capture the relationship between input and output features accuartely generating high error rate on both training and testing dataset. Underfitting is when training accuracy and testing accuracy is low.\n",
    "##### Underfitting can occur when our model needs more training time, more input features, less regularization i.e. when the model is too simple. It occurs when our model has high bias and high or low variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1899f3-3bbc-45d2-94f6-177121d3ab7b",
   "metadata": {},
   "source": [
    "#### (4)-\n",
    "##### Bias-variance tradeoff- There is a tradeoff between a model’s ability to minimize bias and variance which is referred to as the best solution for selecting a value of Regularization constant. It refers to the trade-off between bias and variance of a model to accurately capture the underlying patterns in the data (low bias) and its ability to generalize well to new, unseen data (low variance). Proper understanding of these errors would help to avoid the overfitting and underfitting of a data set while training the algorithm. \n",
    "##### The bias is known as the difference between the prediction of the values by the ML model and the correct value. High bias leads to underfitting. The variability of model prediction for a given data point which tells us spread of our data is called the variance of the model. High variance leads to overfitting. \n",
    "##### If the algorithm is too simple (i.e. underfitting) then it may be on high bias and low variance condition and thus is error-prone. If algorithms is too complex (i.e. overfitting) then it may be on high variance and low bias. In the latter condition, the new entries will not perform well. Well, there is something between both of these conditions, known as Trade-off or Bias Variance Trade-off which is characterized by low bias and low variance. This tradeoff in complexity is why there is a tradeoff between bias and variance. The bias and variance should be such that the total error is minimized. An optimal balance of bias and variance would never overfit or underfit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b11723-6842-4f85-b24a-b5c97d333a6e",
   "metadata": {},
   "source": [
    "#### (5)-\n",
    "##### Learning curves- are used to effectively identify overfitting and underfitting in machine learning models. Learning curves plot the training and validation loss of a sample of training examples by incrementally adding new training examples. Learning curves help us in identifying whether adding additional training examples would improve the validation score (score on unseen data). If a model is overfit, then adding additional training examples might improve the model performance on unseen data. Similarly, if a model is underfit, then adding training examples doesn’t help.\n",
    "##### Features of learning curve of overfit model-\n",
    "-  Very low training loss that slightly increases on adding more training examples.\n",
    "-  High validation loss in beginning that decreases on adding more training data but doesn't flatten indicating addition of more training examples can improve the model performance on unseen data.\n",
    "-  Training loss and Validation loss are far away from each other.\n",
    "##### Features of learning curve of underfit model-\n",
    "- Low training loss at the beginning which increases on adding more training examples.\n",
    "- High validation loss at the beginning that lowers on adding more training examples and flattens indicating adding more training examples doesn’t improve performance on unseen data.\n",
    "- Training loss and validation loss are close to each other at the end.\n",
    "\n",
    "##### We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data.\n",
    "\n",
    "##### Using cross-validation- Cross-validation is a technique for evaluating the performance of a model on multiple subsets of the training data. If the model performs well on all the subsets, it indicates that the model is not overfitting. If the performance is poor on all subsets, it indicates that the model is underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5b0ff1-9594-4dd8-a770-8907844bf592",
   "metadata": {},
   "source": [
    "#### (6)-\n",
    "#####  There are two types of error in machine learning- Reducible error and Irreducible error. Bias and Variance come under reducible error.\n",
    "##### Bias is simply defined as the difference or error occuring between model's predicted and actual value. Bias is a systematic error that occurs due to wrong assumptions in the machine learning process. \n",
    "##### In machine learning, variance is the amount by which the performance of a predictive model changes when it is trained on different subsets of the training data. More specifically, variance is the variability of the model that how much it is sensitive to another subset of the training dataset. i.e how much it can adjust on the new subset of the training dataset.\n",
    "##### High bias models are too simple and are not able to capture the dataset trend. They are considered as underfitting models which have a high train and test error rate. High bias models have less parameters than required (too simple).\n",
    "- Example) a linear regression model may have a high bias if the data has a non-linear relationship.\n",
    "##### High variance models are too complex and are able to fit the training data closely, including the noise in the data. They are considered as overfitting models with low train error and high test error. High variance models have more parameters than necessary (high complexity).\n",
    "- Example) Decision trees with deep and complex branches that can fit the training data closely, KNN, SVM etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27bf387-cfc1-48fd-9ca8-34159de71f2d",
   "metadata": {},
   "source": [
    "#### (7)-\n",
    "##### Regularization is a technique used to minimize the adjusted loss function by fitting the function appropriately on the given training set and avoid overfitting. There are two main types of regularization techniques: Ridge Regularization(L2 regularization) and Lasso Regularization(L1 regularization)\n",
    "##### 1- Ridge regularization- Also known as Ridge regression, it modifies the over-fitted model by adding squared magnitude of coefficient as penalty term to the loss function. \n",
    "- This means that the mathematical function representing our machine learning model is minimized and coefficients are calculated. The magnitude of coefficients is squared and added. Ridge Regression performs regularization by shrinking the coefficients present. \n",
    "- The penalty term is represented by Lambda λ. Higher the penalty, lower is the magnitude of coefficients, thus it shrinks the parameters. Therefore, it is used to prevent multicollinearity, and it reduces the model complexity by coefficient shrinkage. \n",
    "##### 2- Lasso regularization- Also known as Lasso regression, it modifies the over-fitted model by adding absolute value of magnitude of coefficient as penalty term to the loss function.\n",
    "- Lasso regression also performs coefficient minimization,  but instead of squaring the magnitudes of the coefficients, it takes the absolute values of coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc04936-00d7-411e-8f02-d8cc32cb9696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
